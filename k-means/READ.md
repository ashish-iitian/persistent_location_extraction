
The folder has 2 scripts. Clustering was done in a 3-d space with time of day, latitude and longitude being the dimensions. The motivation for k-means clustering was that a user would generate lot of LTE data at his persistent location which could form one tight cluster and then, the records generated on the move during the day could form another big cluster (since entries would be apart in location and time).

The first was "scripts-kmeans_spark..." an implementation of kmeans clustering using Pyspark MlLib module's kmeans without any weighting. A subset of entire dataset that had been pickled using cPickle was used for trying this out. 500 most active users were picked so that kmeans had sufficient data to run clustering on. I tried 2 cluster approach first and the performance of this approach was evaluated on the basis of the tightness of cluster (radius of cluster gives deviation in time and location). The centroid gives typical time of day when user is at his persistent location (deviation telling us about the time he spends there) and lat-long value of his location (deviation tells us about the range within which his persistent location is believed to be). It was observed this approach performed poorly.

The next attempt was to implement a weighted k-means "scripts-kmeans_old-weighting..." by using Scipy's kmeans2 because of it being simpler to use. This was slightly complicated. First we got rid of noise by filtering off entries that had a duration less than 5 mins since a user would spend more time at his persistent location. Weighting was implemented to give entries with higher duration more importance so that centroid of cluster would be closer to it. This was done by replicating an entry by a factor depending on number of signal measurements "n" (taken by the geolocation software to arrive at the lat-long pair for the location of user) and the duration of call at that location "d" in seconds, the weight precisely being (n/2 + d/300)/5. After this, clustering was done using k = 3 and then reducing it to k = 2 to obtain the centroid of user. The tightness of cluster was again used to evaluate our confidence in this approach. This approach performed better than Pyspark kmeans implementation but it was poorer than local minima analysis, while being way more complicated in implementation.

P.S. If you are unable to understand why I chose 2 cluster initially and later, I worked with a 3-cluster reduced to 2-cluster approach, please feel free to ask me (my email provided in profile info). For the sake of brevity, I do not go into the specifics here.
